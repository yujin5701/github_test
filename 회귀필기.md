1. 모델을 정의한다: 어떤 식으로 모델을 구성해서, 모델의 정답률을 어떤 지표를 통해 측정할 것이며, 정답률을 높이기 위한 방법은 어떤 식으로 정의할 것인다. 이런 전체적인 디자인을 하는 것.
2. 선형회귀와 GD
- ex) 운동 시간이 늘어나면 체중 감소량도 커진다(선형,Linear 증가. 비례, 반비례 등)
- 앞으로 등장할 새로운 데이터도 한 직선이 추세를 크게 벗어나지 않을 것이라는 합리적인 추측!
- h(x)=wx+b 이라는 가설의 가중치(기울기)와 편향(y절편)을 찾아내는 것이 핵심!
  Cost Function
- 컴퓨터는 선형을 수치화해서 계산해야 함. 즉, 가설 h(x)와 실제값(y)의 차이를 오차라고 하고, 이 오차가 최소화 되는 w와 b를 찾아야 함.
- 전문 용어: 목적함수, 비용함수, 오차함수 등
- 선형회귀에서의 목적함수 = 최소 제곱 오차: 실제값(h)+예측값(h(x))을 제곱하여 모두 더한 형태.

최적화(목적함수의 값을 최소화 할 수 있게 만드는 작업=최적화 알고리즘)
- cost = aw^2 + bw + c
- 경사하강법(Gradient Decent) 알고리즘
- 머신러닝 알고리즘은 가설을 세우고, 실제 데이터와 예측값을 비교하며 적절한 오차함수를 설정한 다음, 이를 최소화할 수 있는 최적화 과정을 거쳐 모델을 만들어 낸다.

3. 이진 분류, 로지스틱 회귀와 cross entropy: 로지스틱 회귀는 사실 분류 문제를 풀 때 사용
4. 시그모이드(sigmoid) 함수
(1) 이진 분류, 시험 점수에 따른 합/불여부 판별: 선형 회귀와 달리 1차방정식으로 표현하기 힘들다. 직선보다 곡선으로 표현 게 나음
(2) 예를 들어서 55점 이하는 탈락아고, 그 이상은 합격일 때, 이걸 직선으로는 표현하기 좋지 않다. 직관적으로 합격 여부를 확률로 표현할 수 있음
(3) h(z) = 1/1+e^(-z), where z = wx + b   => w와 b를 찾아내는 문제로 변형 가능
(4) Cost function, Cross Entropy: 분류 문제에서는 다양한 이유ㅠ로 Cross Entropy라는 비용함수를 사용하는 것이 더 적합.
- 시그모이드 함수의 합수값은 0~1임. 실제값이 1인데 예측값이 0에 가까우면 오차가 커져야 하고, 실제값이 0인데, 예측값이 1에 가깝다면 오차가 커져야 함.
  
4. 다중분류 소프트맥스 회귀
(1) 다중 클래스 분류: 3개 이상의 레이블 값이 존재하는 문제를 다중 클래스라 함.
예) 꼬리 길이, 귀의 너비, 몸통 길이를 특징으로 견종 구분하는 가상의 데이터들을 통해 말티즈, 푸들, 치와와 구분에 대한 각각의 로지스틱 회귀 모델을 훈련
- 말티즈: 0.7
- 푸들: 0.6
- 치와와: 0.3
=> 모두 더했을 때 1이 되어야 하지 않음? => 합이 1이 되도록 하고 싶음.

(2) 소프트맥스 함수: 소프트맥스 회귀에서 사용
입력 값을 0~1 사이의 값으로 출력하며, 전체 출력 값들의 총합이 1이 되도록 만들어주는 함수


분류하고 싶은 클래스의 수만큼 k 설정. 예를 들어 견종 분류에서는 k=3으로 설정됨. (말티즈, 푸들, 치와와). 특징을 조합하여 만든 x1, x2, x3가 존재하고, 소프트맥스 함수를 거쳐 총합이 1이 되는 확률로 반환.

 
(3) One-hot Encoding & Cost function
다중 클래스 분류에서 소프트맥스 회귀 모델을 훈련시키기 위해 로지스틱 회귀의 목적 함수였던 크로스 엔트로피를 활용하면 됨. 하지만 다중 클래스에 그대로 true(1), false(0)를 그대로 적용하면 문제 생김. 
다중 클래스 분류에서는 결과가 3개 이상이다. 말티즈=0, 푸들=1, 치와와=2 이렇게 인코딩 하면 오차 계산 결과가 적절하지 않을 수 있음.






이러한 왜국 방지용으로 다중 클래스 분류에서는 원-핫 인코딩 기법 사용. 벡터값 사용









이제 다시 돌아와서 크로스 엔트로피 사용. K는 클래스의 갯수(소프트맥스 예측값 배열의 크기). Y는 실제 정답(원-핫 인코딩으로 생성된 값), y^는 소프트맥스의 예측값.
 
 
